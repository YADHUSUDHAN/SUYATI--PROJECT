{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "745ef2e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-08T16:01:51.891735Z",
     "iopub.status.busy": "2022-07-08T16:01:51.890822Z",
     "iopub.status.idle": "2022-07-08T16:01:55.940533Z",
     "shell.execute_reply": "2022-07-08T16:01:55.939566Z"
    },
    "papermill": {
     "duration": 4.076646,
     "end_time": "2022-07-08T16:01:55.943547",
     "exception": false,
     "start_time": "2022-07-08T16:01:51.866901",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, cross_val_score, cross_validate, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import pickle\n",
    "import os\n",
    "pd.pandas.set_option('display.max_columns',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d6117b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-08T16:01:55.992720Z",
     "iopub.status.busy": "2022-07-08T16:01:55.992126Z",
     "iopub.status.idle": "2022-07-08T16:01:55.998337Z",
     "shell.execute_reply": "2022-07-08T16:01:55.997477Z"
    },
    "papermill": {
     "duration": 0.03304,
     "end_time": "2022-07-08T16:01:56.000354",
     "exception": false,
     "start_time": "2022-07-08T16:01:55.967314",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81fd26f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-08T16:01:56.048681Z",
     "iopub.status.busy": "2022-07-08T16:01:56.048283Z",
     "iopub.status.idle": "2022-07-08T16:01:56.451170Z",
     "shell.execute_reply": "2022-07-08T16:01:56.450100Z"
    },
    "papermill": {
     "duration": 0.430994,
     "end_time": "2022-07-08T16:01:56.454293",
     "exception": false,
     "start_time": "2022-07-08T16:01:56.023299",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "\n",
    "# Connect to the MySQL database\n",
    "conn = mysql.connector.connect(\n",
    "    host='your_host',\n",
    "    user='your_username',\n",
    "    password='your_password',\n",
    "    database='your_database'\n",
    ")\n",
    "\n",
    "# Create a SQL query to fetch the data\n",
    "query = \"SELECT * FROM your_table\"\n",
    "\n",
    "# Load the data into a DataFrame\n",
    "housing_data = pd.read_sql(query, conn)\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "258ed563",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selected_features = list(housing_data.iloc[:, :-1])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e8887b1",
   "metadata": {},
   "source": [
    "### Splitting the Data (Train & Test Sets):\n",
    "\n",
    "For the purposes of modeling, it will be necessary to split the data into at least training and testing sets. Further splitting may be done for cross validation in the training phase. In order to ensure that the test set is representative of the training set, I'll use stratified sampling to make the test set have the same proportion of price categories defined in the data analysis section (as shown in the pie chart)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed21cb3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Function for standardizing the data, and converting boolean values to numerical.\n",
    "'''\n",
    "def standardize_dataframe(df, scaler=None):\n",
    "    # Convert boolean columns to numerical (0 or 1); and standardize numerical columns\n",
    "    bool_df = df[[col for col in df.columns if df.dtypes[col] == 'bool']].astype('float64')\n",
    "    num_df = df[[col for col in df.columns if df.dtypes[col] == 'float64'\n",
    "                 or df.dtypes[col] == 'int64']]\n",
    "    # If scaler is given, use it. Otherwise train a new scaler on the numerical columns.\n",
    "    if(scaler==None):\n",
    "        scaler = StandardScaler().fit(num_df)\n",
    "    num_df = pd.DataFrame(scaler.transform(num_df), columns=num_df.columns)\n",
    "    \n",
    "    # Ensure that bool_df has ascending indices just like num_df (since scaling resets indices)\n",
    "    bool_df = bool_df.reset_index().drop(['index'], axis=1)\n",
    "    \n",
    "    return num_df.join(bool_df), scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce12e3a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sampling hyperparameters\n",
    "test_size = 0.2\n",
    "intervals = [0, 250000, 400000, 600000, 800000, 1500000, housing_data['latestPrice'].max()]\n",
    "intervals.sort()\n",
    "price_categories = pd.cut(housing_data['latestPrice'], intervals)\n",
    "\n",
    "# Split into training and test sets\n",
    "split_func = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=46)\n",
    "indices = list(split_func.split(housing_data, price_categories))[0]\n",
    "training_data, test_data = housing_data.iloc[indices[0]], housing_data.iloc[indices[1]]\n",
    "x_train, y_train = training_data[selected_features], training_data['latestPrice']\n",
    "x_test, y_test = test_data[selected_features], test_data['latestPrice']\n",
    "y_train, y_test = y_train.reset_index()['latestPrice'], y_test.reset_index()['latestPrice']\n",
    "\n",
    "# Standardize the sets (using only parameters from training set)\n",
    "x_train, scaler = standardize_dataframe(x_train)\n",
    "x_test, _ = standardize_dataframe(x_test, scaler)\n",
    "# Full sets standardized\n",
    "x, y = pd.concat([x_train, x_test], ignore_index=True), pd.concat([y_train, y_test], ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f6fdf6d7",
   "metadata": {},
   "source": [
    "Just to be sure that the test data is in fact representative of the training data, we can compare the price labels in each set using a scatter plot. The plot below shows that the training labels (in blue) and the test labels (in red) overlap pretty well, and that each includes a good sample of outliers from the data. The pie charts to the left show the true proportions of price categories in each set. We can see that they are nearly identical, and thus the test set is in fact representative of the training set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee11b3ab",
   "metadata": {},
   "source": [
    "# Modeling:\n",
    "In this section I'll be creating models to predict the price of a home based on its other features.\n",
    "\n",
    "The cell below defines several functions for saving and loading models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04913e96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_dir = '/home/yadhu_sudhan/MEGA/PROJECT/SUYATI/SUYATI PROJECT/models/'\n",
    "linear_regressor_name = \"linear_regressor\"\n",
    "decision_tree_regressor_name = \"decision_tree_regressor\"\n",
    "random_forest_regressor_name = \"random_forest_regressor\"\n",
    "\n",
    "def save_sklearn_model(model, model_name):\n",
    "    with open(model_dir + model_name + '.pkl', 'wb') as file:  pickle.dump(model, file)\n",
    "\n",
    "def load_sklearn_model(model_name):\n",
    "    with open(model_dir + model_name + '.pkl', 'rb') as file:\n",
    "        model = pickle.load(file)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f5fdff2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model R-squared:\t 0.52\n",
      "Best Model Adjusted R-squared:\t 0.52\n",
      "\n",
      "Saved Model Location:\t/home/yadhu_sudhan/MEGA/PROJECT/SUYATI/SUYATI PROJECT/models/random_forest_regressor.pkl\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor()\n",
    "#best_params = {'n_estimators': 100, 'max_features': 4}\n",
    "#model = RandomForestRegressor(n_estimators=best_params['n_estimators'], max_features=best_params['max_features'])\n",
    "model = RandomForestRegressor(n_estimators=100, max_features=4)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "adjusted_r2 = 1 - (1-r2)*(len(y)-1)/(len(y)-x.shape[1]-1)\n",
    "print(\"Best Model R-squared:\\t\", np.around(r2, 2))\n",
    "print(\"Best Model Adjusted R-squared:\\t\", np.around(adjusted_r2, 2))\n",
    "\n",
    "# Save model\n",
    "save_sklearn_model(model, random_forest_regressor_name)\n",
    "print(\"\\nSaved Model Location:\\t\" + model_dir + random_forest_regressor_name + '.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3ea4a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['livingAreaSqFt','numOfBathrooms','lotSizeSqFt','numOfBedrooms','numOfStories','numOfPhotos','numOfPatioAndPorchFeatures','numOfParkingFeatures','latest_saleyear','numOfSecurityFeatures','hasSpa','hasView']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d65ffa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(housing_data[feature_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7b4e39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import pandas as pd\n",
    "\n",
    "# MySQL database connection details\n",
    "host = 'your_host'\n",
    "user = 'your_username'\n",
    "password = 'your_password'\n",
    "database = 'your_database'\n",
    "\n",
    "# Table name for predictions\n",
    "table_name = 'predictions_table'\n",
    "\n",
    "# Convert predictions to a DataFrame\n",
    "predictions_df = pd.DataFrame({'Prediction': y_pred})\n",
    "\n",
    "# Connect to the MySQL database\n",
    "conn = mysql.connector.connect(\n",
    "    host=host,\n",
    "    user=user,\n",
    "    password=password,\n",
    "    database=database\n",
    ")\n",
    "\n",
    "# Create a cursor\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create the predictions table if it doesn't exist\n",
    "create_table_query = f\"CREATE TABLE IF NOT EXISTS {table_name} (id INT AUTO_INCREMENT PRIMARY KEY, Prediction FLOAT)\"\n",
    "cursor.execute(create_table_query)\n",
    "\n",
    "# Insert the predictions into the database\n",
    "insert_query = f\"INSERT INTO {table_name} (Prediction) VALUES (%s)\"\n",
    "for index, row in predictions_df.iterrows():\n",
    "    cursor.execute(insert_query, (row['Prediction'],))\n",
    "\n",
    "# Commit the changes and close the cursor and connection\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3711.86476,
   "end_time": "2022-07-08T17:03:33.485158",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-07-08T16:01:41.620398",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
